{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e67d2b1-4ea8-46ae-96fc-390291e2be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c826d4-509c-4ff7-8879-8cfd4b69947d",
   "metadata": {},
   "source": [
    "## load csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1460c95a-949e-4010-9561-4ff174806c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \n",
    "\n",
    "id = 'src_subject_id' # enter your id variable \n",
    "\n",
    "year = ['b_', '6mo_', 'y1_', '18mo_', 'y2_', 'y3_', '30mo', '42mo','y4_'] #assign a variable to each year \n",
    "eventcolumn = 'eventname' #this is the year filter variable \n",
    "eventyear = ['baseline_year_1_arm_1', '6_month_follow_up_arm_1', \\\n",
    "             '1_year_follow_up_y_arm_1', '18_month_follow_up_arm_1',\\\n",
    "            '2_year_follow_up_y_arm_1', '3_year_follow_up_y_arm_1', '30_month_follow_up_arm_1', \\\n",
    "            '42_month_follow_up_arm_1','4_year_follow_up_y_arm_1']\n",
    "sex = ['demo_sex_v2'] #1 = male #2 = female #3=intersex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cbd60e-98bd-49b2-89bd-68ec429efb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACES\n",
    "ACE_ksad_ptsd = pd.read_csv(working_dir + '/mental-health/mh_p_ksads_ptsd.csv', low_memory=False)\n",
    "ACE_mnbs = pd.read_csv(working_dir + '/culture-environment/ce_y_mnbs.csv', low_memory=False)\n",
    "ACE_pm = pd.read_csv( working_dir + '/culture-environment/ce_y_pm.csv', low_memory=False)\n",
    "ACE_mh_y_LE = pd.read_csv(working_dir + '/mental-health/mh_y_le.csv', low_memory=False)\n",
    "ACE_fes = pd.read_csv( working_dir + '/culture-environment/ce_y_fes.csv', low_memory=False)\n",
    "ACE_nsc = pd.read_csv( working_dir + '/culture-environment/ce_y_nsc.csv', low_memory=False)\n",
    "ACE_dm = pd.read_csv( working_dir + '/culture-environment/ce_y_dm.csv', low_memory=False)\n",
    "ACE_ksad_bg = pd.read_csv( working_dir + '/mental-health/mh_y_ksads_bg.csv', low_memory=False)\n",
    "ACE_cbb = pd.read_csv( working_dir + '/mental-health/mh_y_cbb.csv', low_memory=False)\n",
    "#general info \n",
    "ACE_demo = pd.read_csv (working_dir + '/abcd-general/abcd_p_demo.csv', low_memory=False)\n",
    "\n",
    "#Protective Factors \n",
    "p_behavior = pd.read_csv (working_dir +'/culture-environment/ce_y_crpbi.csv', low_memory=False)\n",
    "p_friend = pd.read_csv (working_dir + '/culture-environment/ce_y_pnh.csv', low_memory=False) #firneds\n",
    "p_self = pd.read_csv (working_dir + '/culture-environment/ce_y_wps.csv', low_memory=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc621406-2072-493d-a196-1f1a1462d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two DataFrames on a common column (e.g., 'src_subject_id' and 'eventname')\n",
    "Demo_full_df = ACE_mnbs.merge(ACE_ksad_ptsd, on=['src_subject_id', 'eventname'], how='outer').merge(ACE_pm, on=['src_subject_id', 'eventname'], how='outer')\\\n",
    ".merge(ACE_mh_y_LE, on=['src_subject_id', 'eventname' ], how='outer').merge(ACE_fes, on=['src_subject_id', 'eventname'], how='outer')\\\n",
    ".merge(ACE_nsc, on=['src_subject_id', 'eventname'], how='outer').merge(ACE_dm, on=['src_subject_id', 'eventname', ], how='outer')\\\n",
    ".merge(ACE_ksad_bg, on=['src_subject_id', 'eventname'], how='outer').merge(ACE_cbb, on=['src_subject_id', 'eventname'], how='outer')\\\n",
    ".merge(ACE_demo, on=['src_subject_id', 'eventname'], how='outer').merge(p_behavior, on=['src_subject_id', 'eventname'], how='outer')\\\n",
    ".merge(p_friend, on=['src_subject_id', 'eventname'], how='outer').merge(p_self, on=['src_subject_id', 'eventname'], how='outer')\n",
    "\n",
    "#make all variables --> can pull them from df bc merged all of them\n",
    "standard_vars = ['src_subject_id', 'eventname']\n",
    "physical_abuse_vars = [ 'ksads_ptsd_raw_762_p', 'ksads_ptsd_raw_763_p', 'ksads_ptsd_raw_761_p']\n",
    "sexual_abuse_vars = [ 'ksads_ptsd_raw_767_p', 'ksads_ptsd_raw_768_p', 'ksads_ptsd_raw_769_p']\n",
    "neglect_vars = [ 'parent_monitor_q3_y', 'mnbs_doing',  'mnbs_school' , 'mnbs_trouble' , 'mnbs_understand'] #diff scales\n",
    "emotional_vars = ['ksads_ptsd_raw_765_p','ksads_ptsd_raw_764_p']\n",
    "drug_vars = ['ple_sud_y']\n",
    "fam_mh_vars= [ 'ple_suicide_y', 'ple_mh_y'] #mental health problem ask kid \n",
    "dv_vars = ['ksads_ptsd_raw_766_p', 'fes_youth_q6']\n",
    "fam_structure_vars = ['ple_separ_y', 'ple_foster_care_y', 'ple_deported_y'] #foster care and deported in future years \n",
    "fam_death_vars = ['ple_died_y' , 'ple_died_fu2_y' , 'ksads_ptsd_raw_770_p' ,'ple_hospitalized_y' , 'ple_injured_y' , 'ple_friend_injur_y' , 'ple_friend_died_y' , 'ksads_ptsd_raw_770_p' ]\n",
    "neigh_safety_vars = ['neighborhood_crime_y', 'ple_shot_y', 'ple_victim_y', 'ksads_ptsd_raw_760_p'] #diff scales\n",
    "jail_vars = ['ple_arrest_y', 'ple_law_y', 'ple_jail_y']\n",
    "bullying_vars = ['dim_yesno_q2', 'dim_yesno_q3', 'dim_yesno_q1', 'cybb_phenx_harm' , 'ksads_bully_raw_26' ] #diff scales\n",
    "housing_vars = ['ple_homeless_y', 'demo_fam_exp1_v2', 'demo_fam_exp1_v2_l', 'demo_fam_exp4_v2' , 'demo_fam_exp4_v2_l']\n",
    "housing_vars2 = ['demo_fam_exp2_v2', 'demo_fam_exp3_v2', 'demo_fam_exp5_v2','demo_fam_exp2_v2_l', 'demo_fam_exp3_v2_l', 'demo_fam_exp5_v2_l']\n",
    "protect_vars = ['crpbi_parent1_y', 'crpbi_parent2_y' , 'crpbi_parent3_y', 'crpbi_parent4_y', 'crpbi_parent5_y',  'crpbi_caregiver12_y' ,'crpbi_caregiver13_y', 'crpbi_caregiver14_y', 'crpbi_caregiver15_y' ,'crpbi_caregiver16_y' ]\n",
    "protect_vars2 = ['wps_ss_sum' , 'wps_ss_sum_nm' ,'pnh_ss_protective_scale' ,'pnh_encourage', 'pnh_art_involve' , 'pnh_help' , 'pnh_how_much_encourage' , 'pnh_how_much_help' , 'pnh_substance' , 'pnh_ss_protective_scale_nm' , 'crpbi_y_ss_parent_nm' ,'crpbi_y_ss_caregiver_nm', 'crpbi_y_ss_caregiver' ,'crpbi_y_ss_parent']\n",
    "protect_vars_3 = ['wps_q1_y' , 'wps_q2_y', 'wps_q3_y','wps_q4_y','wps_q5_y','wps_q6_y']\n",
    "protect_vars_4 = ['fes_y_ss_fc_pr','fes_y_ss_fc','fes_y_ss_fc_nm','fes_y_ss_fc_nt' ,'fes_y_ss_fc_nt','fes_youth_q1', 'fes_youth_q2', 'fes_youth_q3', 'fes_youth_q4', 'fes_youth_q5', 'fes_youth_q7', 'fes_youth_q8' ,'fes_youth_q9' ,'fes_youth_q10' , 'fes_youth_q11' ,'fes_youth_q12' ,'fes_youth_q13' ,'fes_youth_q14' ,'fes_youth_q15' ,'fes_youth_q16' ,'fes_youth_q17' ,'fes_youth_q18']\n",
    "# make a master list of these 3 mini lists\n",
    "vars_to_pull = standard_vars + physical_abuse_vars + sexual_abuse_vars + neglect_vars + emotional_vars +drug_vars + fam_mh_vars\\\n",
    "+dv_vars + fam_structure_vars + fam_death_vars + neigh_safety_vars + jail_vars +bullying_vars + housing_vars + protect_vars  \\\n",
    " + protect_vars2 + housing_vars2 + protect_vars_3 +protect_vars_4\n",
    "\n",
    "ACE_df = Demo_full_df [vars_to_pull]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756150f4-b20b-4397-8993-30a691d9554c",
   "metadata": {},
   "source": [
    "## calculate ACEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8755ff-7dc9-488d-a072-190c523bc617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the column names of the DataFrame\n",
    "print(ACE_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e648197-470e-486b-890b-e699bd8bd9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'ple_died_y' , 'ple_died_fu2_y' , 'ksads_ptsd_raw_770_p' \n",
    "#demo_fam_exp2_v2', 'demo_fam_exp3_v2', 'demo_fam_exp5_v2','demo_fam_exp2_v2_l', 'demo_fam_exp3_v2_l', 'demo_fam_exp5_v2_l'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858bbb40-0aeb-403d-b5d2-ab8a0c4149b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"starting to calculate your ACE data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5f3715-25c5-4269-bb65-e2360b921c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ACE_df.groupby('src_subject_id').size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dd0252-d35b-46d7-86e4-2ce8ffa905f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" IMPORTANT THIS CODE IS RESTRICED TO BASELINE; Y1; Y2\")\n",
    "\n",
    "ACE_df = ACE_df[~ACE_df['eventname'].isin(['3_year_follow_up_y_arm_1', '4_year_follow_up_y_arm_1'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c95bfb-b613-4a63-82b8-bf1fd900e4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ACEs(df, vars_to_check, new_col_name, col_count, sum_col_name, sum_col_count):\n",
    "    \"\"\" \n",
    "    Process a group of variables to check for a dataframe and create a new column.\n",
    "    df - dataframe name\n",
    "    vars_to_check - list of variables\n",
    "    new_col_name - name for the new column to create\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select the first variable to check from the vars_to_check list\n",
    "    vars = vars_to_check[0]  # or any specific variable you want to check\n",
    "    print(vars)\n",
    "\n",
    "    # Get the unique event names for the non-null values of the selected variable\n",
    "    possible_events = df.dropna(subset=[vars])['eventname'].unique().tolist()\n",
    "    print(possible_events)\n",
    "\n",
    "\n",
    "    # Define a helper function to process each row\n",
    "    def process_row(row):\n",
    "        \n",
    "\n",
    "        # Check if any value in vars_to_check is 1\n",
    "        if (row[vars_to_check] == 1).any():\n",
    "            return 1\n",
    "        \n",
    "        # Check if all values in vars_to_check are 0\n",
    "        if (row[vars_to_check] == 0).all():\n",
    "            return 0\n",
    "\n",
    "                # Check for missing values in the vars_to_check columns\n",
    "        if row[vars_to_check].isnull().any():\n",
    "            return 3  # Missing value in any column\n",
    "        \n",
    "        \n",
    "        # If none of the above, return None or other specific values\n",
    "        return None\n",
    "    \n",
    "    # Apply the process_row function to each row in the dataframe\n",
    "    df[new_col_name] = df.apply(process_row, axis=1)\n",
    "\n",
    "     # Create the subj_dict for flag_1 (subjects with flag == 1)\n",
    "    subj_dict = {\n",
    "        'flag_1': df['src_subject_id'][df[new_col_name] == 1].tolist()\n",
    "    }\n",
    "\n",
    "    # Create the ACE_PA_count column and set it based on whether 'src_subject_id' is in 'flag_1'\n",
    "    df[col_count] = df['src_subject_id'].isin(subj_dict['flag_1']).astype(int)\n",
    "\n",
    "    \n",
    "      # Create the flag_0_ids list based on the new_col_name == 0 condition across all years of data\n",
    "    flag_0_ids = (df[(df['eventname'].isin([possible_events])) & (df[new_col_name] == 0)]\n",
    "                  .groupby('src_subject_id')['eventname']\n",
    "                  .nunique())\n",
    "    flag_0_ids = flag_0_ids[flag_0_ids == len(possible_events)].index.tolist()  # Subjects with flag == 0 for both baseline and year2\n",
    "\n",
    "    \n",
    "    \n",
    "    # For 'flag_0', we need subjects who have flag == 0 for all event years tested \n",
    "    df[col_count] = np.where(df['src_subject_id'].isin(flag_0_ids), 0, df[col_count])\n",
    "\n",
    "\n",
    "    def process_summary(row, vars_to_check):\n",
    "        # Check if any value in vars_to_check is 1\n",
    "        if (row[vars_to_check] == 1).any():\n",
    "            return row[vars_to_check].sum()  # Sum all values in vars_to_check for the row\n",
    "        \n",
    "        # Check if all values in vars_to_check are 0\n",
    "        if (row[vars_to_check] == 0).all():\n",
    "            return 0\n",
    "    \n",
    "        # If none of the above conditions are met, return None or other default value\n",
    "        return None\n",
    "    \n",
    "    # Apply the process_summary function to each row\n",
    "    df[sum_col_name] = df.apply(process_summary, axis=1, vars_to_check=vars_to_check)  \n",
    "    \n",
    " \n",
    "        # Create the dictionary where keys are 'src_subject_id' and values are True/False\n",
    "    subj_dict2 = dict(zip(df[df[sum_col_name] >= 1]['src_subject_id'], df[df[sum_col_name] >= 1][sum_col_name]))\n",
    "\n",
    "\n",
    "    # # Create the ACE_PA_count column and set it based on whether 'src_subject_id' is in 'subj_dict'\n",
    "    df[sum_col_count] = df['src_subject_id'].map(subj_dict2)\n",
    "\n",
    "\n",
    "    # Step 3: Check if a subject has 0 across all years (rows) and update the column if true\n",
    "    # Group by 'src_subject_id' and check if all 'ACE_sum_count' are 0 for each subject\n",
    "    subjects_with_zero = df.groupby('src_subject_id')[sum_col_count].transform('sum') == 0\n",
    "    \n",
    "    # Step 4: Update the 'sum_col_count' to 0 for subjects that have zero in all years (rows)\n",
    "    df[sum_col_count] = df[sum_col_count].where(~subjects_with_zero, 0)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9daee03-4a37-4dce-a8cc-9130c77ac7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## turning off warning \n",
    "import warnings\n",
    "\n",
    "# Import SettingWithCopyWarning from pandas.errors\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "\n",
    "# Suppress the warning\n",
    "warnings.simplefilter(action='ignore', category=SettingWithCopyWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f85784-510f-4542-8001-c26d2fe012a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variables to check\n",
    "vars_to_check = ['ksads_ptsd_raw_763_p', 'ksads_ptsd_raw_762_p', 'ksads_ptsd_raw_761_p']\n",
    "# Column name to store the result\n",
    "col_name = 'ACE_PA'\n",
    "col_count = 'ACE_PA_count'\n",
    "sum_col_name = 'ACE_PA_sum'\n",
    "sum_col_count = 'ACE_PA_count'\n",
    "\n",
    "\n",
    "# Call the function to create the new column\n",
    "ACE_df = calculate_ACEs(ACE_df, vars_to_check, col_name, col_count, sum_col_name, sum_col_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bc2aa9-7965-43df-b566-085ae14ca7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACE_PA tells you how many get a 1 and how many get a 0, but the ACE_PA_count collapses across years. \n",
    "#Bring forward ACEs in baseline and gives a 0 to only those kids who have a 0 for ACEs at baseline and year 2\n",
    "#ACE_sum and ACE_sum_count are for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d24e649-7065-4e47-9d08-5f8b36e1bf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HERE IS THE ACE_PA\\n\")\n",
    "print(ACE_df.groupby('eventname')['ACE_PA'].value_counts())\n",
    "\n",
    "print(\"\\n\\nHERE IS THE ACE_PA_COUNT\\n\")\n",
    "print(ACE_df.groupby('eventname')['ACE_PA_count'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b71a02-b083-46f5-b3b2-a74a788fb4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##NOW RUNNING FOR ALL ACES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f193eefe-1584-4429-8e55-651cb2dcd22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### physical abuse ######################\n",
    "vars_to_check = ['ksads_ptsd_raw_763_p', 'ksads_ptsd_raw_762_p', 'ksads_ptsd_raw_761_p']\n",
    "# Column name to store the result\n",
    "col_name = 'ACE_PA'\n",
    "col_count = 'ACE_PA_count'\n",
    "sum_col_name = 'ACE_PA_sum'\n",
    "sum_col_count = 'ACE_PA_sum_count'\n",
    "\n",
    "\n",
    "# Call the function to create the new column\n",
    "ACE_df = calculate_ACEs(ACE_df, vars_to_check, col_name, col_count, sum_col_name, sum_col_count)\n",
    "\n",
    "###################### sexual abuse ######################\n",
    "vars_to_check = ['ksads_ptsd_raw_767_p', 'ksads_ptsd_raw_768_p', 'ksads_ptsd_raw_769_p']\n",
    "# Column name to store the result\n",
    "col_name = 'ACE_SA'\n",
    "col_count = 'ACE_SA_count'\n",
    "sum_col_name =  'ACE_SA_sum'\n",
    "sum_col_count =  'ACE_SA_sum_count'\n",
    "# Call the function to create the new column\n",
    "ACE_df = calculate_ACEs(ACE_df, vars_to_check, col_name, col_count, sum_col_name, sum_col_count)\n",
    "\n",
    "###################### emotional abuse ######################\n",
    "vars_to_check = ['ksads_ptsd_raw_765_p']\n",
    "# Column name to store the result\n",
    "col_name = 'ACE_EA'\n",
    "col_count = 'ACE_EA_count'\n",
    "sum_col_name = 'ACE_EA_sum'\n",
    "sum_col_count = 'ACE_EA_sum_count'\n",
    "# Call the function to create the new column\n",
    "ACE_df = calculate_ACEs(ACE_df, vars_to_check, col_name, col_count, sum_col_name, sum_col_count)\n",
    "###################### Substance Abuse ######################\n",
    "vars_to_check = ['ple_sud_y']\n",
    "# Column name to store the result\n",
    "col_name = 'ACE_D'\n",
    "col_count = 'ACE_D_count'\n",
    "sum_col_name = 'ACE_D_sum'\n",
    "sum_col_count = 'ACE_D_sum_count'\n",
    "# Call the function to create the new column\n",
    "ACE_df = calculate_ACEs(ACE_df, vars_to_check, col_name, col_count, sum_col_name, sum_col_count)\n",
    "\n",
    "###################### Mental Health ######################\n",
    "vars_to_check = ['ple_mh_y']\n",
    "# Column name to store the result\n",
    "col_name = 'ACE_MH'\n",
    "col_count = 'ACE_MH_count'\n",
    "sum_col_name = 'ACE_MH_sum'\n",
    "sum_col_count = 'ACE_MH_sum_count'\n",
    "# Call the function to create the new column\n",
    "ACE_df = calculate_ACEs(ACE_df, vars_to_check, col_name, col_count, sum_col_name, sum_col_count)\n",
    "\n",
    "###################### Domestic violence ######################\n",
    "vars_to_check = ['ksads_ptsd_raw_766_p', 'fes_youth_q6']\n",
    "# Column name to store the result\n",
    "col_name = 'ACE_DV'\n",
    "col_count = 'ACE_DV_count'\n",
    "sum_col_name = 'ACE_DV_sum'\n",
    "sum_col_count = 'ACE_DV_sum_count'\n",
    "# Call the function to create the new column\n",
    "ACE_df = calculate_ACEs(ACE_df, vars_to_check, col_name, col_count, sum_col_name, sum_col_count)\n",
    "\n",
    "###################### Domestic violence_2 ######################\n",
    "#no FES\n",
    "vars_to_check = ['ksads_ptsd_raw_766_p']\n",
    "# Column name to store the result\n",
    "col_name = 'ACE_DV_2'\n",
    "col_count = 'ACE_DV_count_2'\n",
    "sum_col_name = 'ACE_DV_sum_2'\n",
    "sum_col_count = 'ACE_DV_sum_count_2'\n",
    "# Call the function to create the new column\n",
    "ACE_df = calculate_ACEs(ACE_df, vars_to_check, col_name, col_count, sum_col_name, sum_col_count)\n",
    "\n",
    "\n",
    "\n",
    "###################### family structure ######################\n",
    "vars_to_check = ['ple_separ_y']\n",
    "# Column name to store the result\n",
    "col_name = 'ACE_F'\n",
    "col_count = 'ACE_F_count'\n",
    "sum_col_name = 'ACE_F_sum'\n",
    "sum_col_count = 'ACE_F_sum_count'\n",
    "# Call the function to create the new column\n",
    "ACE_df = calculate_ACEs(ACE_df, vars_to_check, col_name, col_count, sum_col_name, sum_col_count)\n",
    "\n",
    "\n",
    "###################### Jail ######################\n",
    "vars_to_check = ['ple_arrest_y', 'ple_jail_y']\n",
    "# Column name to store the result\n",
    "col_name = 'ACE_J'\n",
    "col_count = 'ACE_J_count'\n",
    "sum_col_name = 'ACE_J_sum'\n",
    "sum_col_count = 'ACE_J_sum_count'\n",
    "# Call the function to create the new column\n",
    "ACE_df = calculate_ACEs(ACE_df, vars_to_check, col_name, col_count, sum_col_name, sum_col_count)\n",
    "\n",
    "###################### Jail ######################\n",
    "vars_to_check = [ 'ple_jail_y']\n",
    "# Column name to store the result\n",
    "col_name = 'ACE_J2'\n",
    "col_count = 'ACE_J2_count'\n",
    "sum_col_name = 'ACE_J2_sum'\n",
    "sum_col_count = 'ACE_J2_sum_count'\n",
    "# Call the function to create the new column\n",
    "ACE_df = calculate_ACEs(ACE_df, vars_to_check, col_name, col_count, sum_col_name, sum_col_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93359196-2881-4672-b57a-6b0aafed3cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Substance Abuse ######################\n",
    "vars_to_check = ['ple_sud_y']\n",
    "# Column name to store the result\n",
    "col_name = 'ACE_D'\n",
    "col_count = 'ACE_D_count'\n",
    "sum_col_name = 'ACE_D_sum'\n",
    "sum_col_count = 'ACE_D_sum_count'\n",
    "# Call the function to create the new column\n",
    "ACE_df = calculate_ACEs(ACE_df, vars_to_check, col_name, col_count, sum_col_name, sum_col_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f0d7c4-d510-49d2-87ed-c7f85e3dac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############friend died#############\n",
    "vars_to_check = ['ple_friend_died_y']\n",
    "# Column name to store the result\n",
    "col_name = 'ACE_FD'\n",
    "col_count = 'ACE_FD_count'\n",
    "sum_col_name = 'ACE_FD_sum'\n",
    "sum_col_count = 'ACE_FD_sum_count'\n",
    "# Call the function to create the new column\n",
    "ACE_df = calculate_ACEs(ACE_df, vars_to_check, col_name, col_count, sum_col_name, sum_col_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b24e75f-cae9-4083-b259-81e1cf9624b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### bullying ######################\n",
    "vars_to_check = ['dim_yesno_q1', 'dim_yesno_q2', 'dim_yesno_q3', 'cybb_phenx_harm','dim_yesno_q1', 'dim_yesno_q2', 'dim_yesno_q3']\n",
    "for v in vars_to_check:\n",
    "    ACE_df[v] = ACE_df[v].replace(888, np.nan)\n",
    "    ACE_df[v] = ACE_df[v].replace(777, np.nan)\n",
    "    ACE_df[v] = ACE_df[v].replace(999, np.nan)\n",
    "\n",
    "    \n",
    "# Column name to store the result\n",
    "col_name = 'ACE_B'\n",
    "col_count = 'ACE_B_count'\n",
    "sum_col_name= 'ACE_B_sum'\n",
    "sum_col_count = 'ACE_B_sum_count'\n",
    "# Call the function to create the new column\n",
    "ACE_df = calculate_ACEs(ACE_df, vars_to_check, col_name, col_count, sum_col_name, sum_col_count)\n",
    "\n",
    "##COUNT 777 as missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f0553b-1374-4f69-af39-d9c1229c3d7c",
   "metadata": {},
   "source": [
    "## collapse demographic longitudinal into 1 variable for ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f643036e-96a3-4cb1-9d72-7e98d4bba70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for n in range(1,6):\n",
    "    #df['demo_fam_exp'+ str(n)] = df['demo_fam_exp'+str(n)+'_v2'].fillna(0) + df['demo_fam_exp'+str(n)+'_v2_l'].fillna(0)\n",
    "    #df = df.drop(columns={'demo_fam_exp'+str(n)+'_v2', 'demo_fam_exp'+str(n)+'_v2_l'})\n",
    "    ACE_df.loc[:,'demo_fam_exp'+ str(n)] = ACE_df.loc[:,['demo_fam_exp'+str(n)+'_v2','demo_fam_exp'+str(n)+'_v2_l']].sum(axis=1, min_count=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527242af-dabb-42a5-b01b-6534a5c8f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### SS/housing ######################\n",
    "vars_to_check = ['demo_fam_exp1', 'demo_fam_exp4', 'demo_fam_exp2', 'demo_fam_exp3', 'demo_fam_exp5']\n",
    "for v in vars_to_check:\n",
    "    ACE_df[v] = ACE_df[v].replace(777, np.nan)\n",
    "\n",
    "# Column name to store the result\n",
    "col_name = 'ACE_SS'\n",
    "col_count = 'ACE_SS_count'\n",
    "sum_col_name ='ACE_SS_sum'\n",
    "sum_col_count = 'ACE_SS_sum_count'\n",
    "# Call the function to create the new column\n",
    "ACE_df = calculate_ACEs(ACE_df, vars_to_check, col_name, col_count, sum_col_name, sum_col_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0a3d38-a4b3-4598-b293-e746b9da1d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "##rewrite for neighborhood and neglect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6e9630-2364-4485-9176-fcc8a3ce9b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ACEs(df, vars_to_check, new_col_name, col_count,sum_col_name, sum_col_count):\n",
    "    \"\"\" \n",
    "    Process a group of variables to check for a dataframe and create a new column.\n",
    "    df - dataframe name\n",
    "    vars_to_check - list of variables\n",
    "    new_col_name - name for the new column to create\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select the first variable to check from the vars_to_check list\n",
    "    vars = vars_to_check[0]  # or any specific variable you want to check\n",
    "    print(vars)\n",
    "\n",
    "    # Get the unique event names for the non-null values of the selected variable\n",
    "    possible_events = df.dropna(subset=[vars])['eventname'].unique().tolist()\n",
    "    print(possible_events)\n",
    "\n",
    "\n",
    "    # Define a helper function to process each row\n",
    "    def process_row(row):\n",
    "        \n",
    "\n",
    "        # Check if any value in vars_to_check is 1\n",
    "        if ((row[vars_to_check] == 1) | (row[vars_to_check] == 2)).any():\n",
    "            return 1\n",
    "\n",
    "\n",
    "        for var in vars_to_check:\n",
    "            # For variables on a scale of 0-1, check if they are 0\n",
    "            if df[var].max() == 1:\n",
    "                # Check if the value is 0\n",
    "                if (row[var] == 0):\n",
    "                    return 0\n",
    "            # For variables on a scale of 1-5, check if they are >= 3\n",
    "            elif df[var].max() == 5:\n",
    "                # Check if the value is >= 3\n",
    "                if (row[var] >= 3):\n",
    "                    return 0\n",
    "\n",
    "        \n",
    "\n",
    "                # Check for missing values in the vars_to_check columns\n",
    "        if row[vars_to_check].isnull().any():\n",
    "            return 3  # Missing value in any column\n",
    "        \n",
    "        \n",
    "        # If none of the above, return None or other specific values\n",
    "        return None\n",
    "    \n",
    "    # Apply the process_row function to each row in the dataframe\n",
    "    df[new_col_name] = df.apply(process_row, axis=1)\n",
    "    \n",
    "    # Create the subj_dict for flag_1 (subjects with flag == 1)\n",
    "    subj_dict = {\n",
    "        'flag_1': df['src_subject_id'][df[new_col_name] == 1].tolist()\n",
    "    }\n",
    "\n",
    "    # Create the ACE_PA_count column and set it based on whether 'src_subject_id' is in 'flag_1'\n",
    "    df[col_count] = df['src_subject_id'].isin(subj_dict['flag_1']).astype(int)\n",
    "\n",
    "    \n",
    "      # Create the flag_0_ids list based on the new_col_name == 0 condition across all years of data\n",
    "    flag_0_ids = (df[(df['eventname'].isin([possible_events])) & (df[new_col_name] == 0)]\n",
    "                  .groupby('src_subject_id')['eventname']\n",
    "                  .nunique())\n",
    "    flag_0_ids = flag_0_ids[flag_0_ids == len(possible_events)].index.tolist()  # Subjects with flag == 0 for both baseline and year2\n",
    "\n",
    "    \n",
    "    \n",
    "    # For 'flag_0', we need subjects who have flag == 0 for all event years tested \n",
    "    df[col_count] = np.where(df['src_subject_id'].isin(flag_0_ids), 0, df[col_count])\n",
    "\n",
    "\n",
    "    def process_summary(row, vars_to_check):\n",
    "        total_points = 0\n",
    "        \n",
    "        for var in vars_to_check:\n",
    "            value = row[var]\n",
    "            \n",
    "            # For variables on a scale of 1-5 (assign 1 point for 1 or 2)\n",
    "            if 1 <= value <= 5:\n",
    "                if value == 1 or value == 2:\n",
    "                    total_points += 1  # Give 1 point for 1 or 2\n",
    "            \n",
    "            # For variables on a scale of 0-1 (assign 1 point for 1)\n",
    "            elif value == 1:\n",
    "                total_points += 1  # Give 1 point for 1\n",
    "            \n",
    "            # You can add any other conditions here if needed\n",
    "        \n",
    "        # If all values in vars_to_check are 0, return 0\n",
    "        if (row[vars_to_check] == 0).all():\n",
    "            return 0\n",
    "        \n",
    "        # Return the total points for the row\n",
    "        return total_points if total_points > 0 else None\n",
    "        \n",
    "        # Apply the process_summary function to each row\n",
    "    df[sum_col_name] = df.apply(process_summary, axis=1, vars_to_check=vars_to_check)\n",
    "        # Create the dictionary where keys are 'src_subject_id' and values are True/False\n",
    "    subj_dict2 = dict(zip(df[df[sum_col_name] >= 1]['src_subject_id'], df[df[sum_col_name] >= 1][sum_col_name]))\n",
    "\n",
    "\n",
    "    # # Create the ACE_PA_count column and set it based on whether 'src_subject_id' is in 'subj_dict'\n",
    "    df[sum_col_count] = df['src_subject_id'].map(subj_dict2)\n",
    "\n",
    "\n",
    "    # Step 3: Check if a subject has 0 across all years (rows) and update the column if true\n",
    "    # Group by 'src_subject_id' and check if all 'ACE_sum_count' are 0 for each subject\n",
    "    subjects_with_zero = df.groupby('src_subject_id')[sum_col_count].transform('sum') == 0\n",
    "    \n",
    "    # Step 4: Update the 'sum_col_count' to 0 for subjects that have zero in all years (rows)\n",
    "    df[sum_col_count] = df[sum_col_count].where(~subjects_with_zero, 0)\n",
    "\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b38d17e-5dfa-42ab-b9f8-7d394f011997",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Neighborhood Violence ######################\n",
    "vars_to_check = ['neighborhood_crime_y', 'ple_victim_y']\n",
    "# Column name to store the result\n",
    "col_name = 'ACE_NV'\n",
    "col_count = 'ACE_NV_count'\n",
    "sum_col_name = 'ACE_NV_sum'\n",
    "sum_col_count = 'ACE_NV_sum_count'\n",
    "# Call the function to create the new column\n",
    "ACE_df = calculate_ACEs(ACE_df, vars_to_check, col_name, col_count, sum_col_name, sum_col_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78df112b-be14-466e-a9b9-5dbc259d97d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Neglect ######################\n",
    "vars_to_check = ['parent_monitor_q3_y']\n",
    "# Column name to store the result\n",
    "col_name = 'ACE_N'\n",
    "col_count = 'ACE_N_count'\n",
    "sum_col_name = 'ACE_N_sum'\n",
    "sum_col_name = 'ACE_N_sum_count'\n",
    "# Call the function to create the new column\n",
    "ACE_df = calculate_ACEs(ACE_df, vars_to_check, col_name, col_count, sum_col_name, sum_col_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b4b475-af77-4016-8be6-b3647ba2ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of ACE variables to calculate value counts for\n",
    "ace_columns_3 = ['ACE_PA', 'ACE_SA', 'ACE_N', 'ACE_EA', 'ACE_D', 'ACE_MH', \n",
    "               'ACE_DV', 'ACE_F', 'ACE_NV', 'ACE_J2', 'ACE_B', 'ACE_SS']\n",
    "\n",
    "# Loop through the ACE columns, calculate value counts, and count how many times '3' appears\n",
    "for col in ace_columns_3:\n",
    "    print(f\"\\n{col} Value Counts (for 2_year_follow_up_y_arm_1 event):\")\n",
    "    \n",
    "    # Filter the DataFrame for the specific event name\n",
    "    filtered_data = ACE_df[ACE_df['eventname'] == '2_year_follow_up_y_arm_1']\n",
    "    \n",
    "    # Calculate value counts\n",
    "    value_counts = filtered_data[col].value_counts()\n",
    "    print(value_counts)\n",
    "    \n",
    "    # Count how many times '3' appears in the column\n",
    "    count_3 = (filtered_data[col] == 3).sum()\n",
    "    print(f\"Number of '3's in {col}: {count_3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35013496-14e6-449e-9657-1a66c759ed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of ACE variables to calculate value counts for\n",
    "ace_columns_3 = ['ACE_PA', 'ACE_SA', 'ACE_N', 'ACE_EA', 'ACE_D', 'ACE_MH', \n",
    "                 'ACE_DV', 'ACE_F', 'ACE_NV', 'ACE_J2', 'ACE_B', 'ACE_SS']\n",
    "\n",
    "# Filter the DataFrame for the specific event ('2_year_follow_up_y_arm_1')\n",
    "filtered_df = ACE_df[ACE_df['eventname'] == '2_year_follow_up_y_arm_1']\n",
    "\n",
    "# Apply a function to count the number of '3's across the specified ACE columns for each subject\n",
    "filtered_df['total_3s'] = filtered_df[ace_columns_3].apply(lambda row: (row == 3).sum(), axis=1)\n",
    "\n",
    "# Find how many unique subject IDs have more than 1 '3' across all ACE columns for this specific event\n",
    "subjects_with_more_than_1_3 = filtered_df[filtered_df['total_3s'] > 1]['src_subject_id'].unique()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of unique subject IDs with more than 1 '3' across all ACE columns (for 2_year_follow_up_y_arm_1 event): {len(subjects_with_more_than_1_3)}\")\n",
    "print(f\"Subject IDs with more than 1 '3' across all ACE columns (for 2_year_follow_up_y_arm_1 event): {subjects_with_more_than_1_3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c04dbc-945f-43db-8f62-206e1a3e5756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of ACE variables to calculate value counts for\n",
    "ace_columns = ['ACE_PA_count', 'ACE_SA_count', 'ACE_N_count', 'ACE_EA_count', 'ACE_D_count', 'ACE_MH_count', \n",
    "               'ACE_DV_count', 'ACE_F_count', 'ACE_NV_count', 'ACE_J2_count', 'ACE_B_count', 'ACE_SS_count']\n",
    "#ACE_FD_count'\n",
    "# Loop through the ACE columns, calculate value counts, and print them\n",
    "for col in ace_columns:\n",
    "    print(f\"\\n{col} Value Counts:\")\n",
    "    print(ACE_df[ACE_df['eventname'] == '2_year_follow_up_y_arm_1'][col].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdb6e1c-2276-496d-a6d6-8c512a4cc3c8",
   "metadata": {},
   "source": [
    "## create protective scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cd3762-9e3e-467f-8070-f5771071168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "protect_vars_p = ['crpbi_parent1_y', 'crpbi_parent2_y', 'crpbi_parent3_y', 'crpbi_parent4_y', 'crpbi_parent5_y']\n",
    "# Create 'protect_parent' score for Year 1\n",
    "ACE_df['protect_parent_recent'] = ACE_df.apply(\n",
    "    lambda row: row[protect_vars_p].sum() if (row['eventname'] == '1_year_follow_up_y_arm_1' and row[protect_vars_p].notna().sum() >= 4) else None, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Create the maximum 'protect_parent' score for each subject\n",
    "ACE_df['protect_parent'] = ACE_df[protect_vars_p].apply(\n",
    "    lambda row: row.sum() if row.notna().sum() >= 4 else None, axis=1\n",
    ")\n",
    "\n",
    "# Filter the DataFrame to include only the specified event names\n",
    "valid_events = ['1_year_follow_up_y_arm_1', '2_year_follow_up_y_arm_1', 'baseline_year_1_arm_1']\n",
    "filtered_df = ACE_df[ACE_df['eventname'].isin(valid_events)]\n",
    "\n",
    "# Group by subject and calculate the maximum 'protect_self' score for the valid events\n",
    "ACE_df['protect_parent_max'] = filtered_df.groupby('src_subject_id')['protect_parent'].transform('max')\n",
    "\n",
    "# Display the first few rows to verify the result\n",
    "print(ACE_df[['src_subject_id', 'eventname', 'protect_parent_recent', 'protect_parent','protect_parent_max'] + protect_vars_p].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4a88fb-2df5-44ae-8c52-b41db09880c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "protect_vars_f = ['pnh_help', 'pnh_encourage']\n",
    "protect_vars_f2 = ['pnh_help', 'pnh_how_much_help', 'pnh_encourage', 'pnh_how_much_encourage']\n",
    "# Create 'protect_friend' score for Year 1\n",
    "ACE_df['protect_friend_recent'] = ACE_df.apply(\n",
    "    lambda row: row[protect_vars_f].sum() if (row['eventname'] == '2_year_follow_up_y_arm_1' and row[protect_vars_f].notna().sum() == 2) else None, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Create the maximum 'protect_parent' score for each subject\n",
    "ACE_df['protect_friend'] = ACE_df[protect_vars_f].apply(\n",
    "    lambda row: row.sum() if row.notna().sum() == 2 else None, axis=1\n",
    ")\n",
    "\n",
    "# Filter the DataFrame to include only the specified event names\n",
    "valid_events = ['1_year_follow_up_y_arm_1', '2_year_follow_up_y_arm_1', 'baseline_year_1_arm_1']\n",
    "filtered_df = ACE_df[ACE_df['eventname'].isin(valid_events)]\n",
    "\n",
    "# Group by subject and calculate the maximum 'protect_self' score for the valid events\n",
    "ACE_df['protect_friend_max'] = filtered_df.groupby('src_subject_id')['protect_friend'].transform('max')\n",
    "\n",
    "# Display the first few rows to verify the result\n",
    "print(ACE_df[['src_subject_id', 'eventname', 'protect_friend_recent', 'protect_friend', 'protect_friend_max'] + protect_vars_f].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0313edb6-da9e-45f1-8db9-d6215913d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "protect_vars_f = ['pnh_help', 'pnh_encourage']\n",
    "protect_vars_f2 = ['pnh_help', 'pnh_how_much_help', 'pnh_encourage', 'pnh_how_much_encourage']\n",
    "# Create 'protect_friend' score for Year 1\n",
    "ACE_df['protect_friend_recent2'] = ACE_df.apply(\n",
    "    lambda row: row[protect_vars_f2].sum() if (row['eventname'] == '2_year_follow_up_y_arm_1' and row[protect_vars_f2].notna().sum() >= 2) else None, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Create the maximum 'protect_parent' score for each subject\n",
    "ACE_df['protect_friend2'] = ACE_df[protect_vars_f2].apply(\n",
    "    lambda row: row.sum() if row.notna().sum() >= 2 else None, axis=1\n",
    ")\n",
    "\n",
    "# Filter the DataFrame to include only the specified event names\n",
    "valid_events = ['1_year_follow_up_y_arm_1', '2_year_follow_up_y_arm_1', 'baseline_year_1_arm_1']\n",
    "filtered_df = ACE_df[ACE_df['eventname'].isin(valid_events)]\n",
    "\n",
    "# Group by subject and calculate the maximum 'protect_self' score for the valid events\n",
    "ACE_df['protect_friend_max2'] = filtered_df.groupby('src_subject_id')['protect_friend2'].transform('max')\n",
    "\n",
    "# Display the first few rows to verify the result\n",
    "print(ACE_df[['src_subject_id', 'eventname', 'protect_friend_recent2', 'protect_friend2', 'protect_friend_max2'] + protect_vars_f2].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba834224-eb36-435a-b318-282d0f7816bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "protect_vars_s = ['wps_q1_y' , 'wps_q2_y', 'wps_q3_y','wps_q4_y','wps_q5_y','wps_q6_y']\n",
    "\n",
    "# Create 'protect_self' score for Year 1\n",
    "ACE_df['protect_self_recent'] = ACE_df.apply(\n",
    "    lambda row: row[protect_vars_s].sum() if (row['eventname'] == '1_year_follow_up_y_arm_1' and row[protect_vars_s].notna().sum() >= 4) else None, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Create the maximum 'protect_parent' score for each subject\n",
    "ACE_df['protect_self'] = ACE_df[protect_vars_s].apply(\n",
    "    lambda row: row.sum() if row.notna().sum() >= 4 else None, axis=1\n",
    ")\n",
    "\n",
    "# Filter the DataFrame to include only the specified event names\n",
    "valid_events = ['1_year_follow_up_y_arm_1', '2_year_follow_up_y_arm_1', 'baseline_year_1_arm_1']\n",
    "filtered_df = ACE_df[ACE_df['eventname'].isin(valid_events)]\n",
    "\n",
    "# Group by subject and calculate the maximum 'protect_self' score for the valid events\n",
    "ACE_df['protect_self_max'] = filtered_df.groupby('src_subject_id')['protect_self'].transform('max')\n",
    "\n",
    "# Display the first few rows to verify the result\n",
    "print(ACE_df[['src_subject_id', 'eventname', 'protect_self', 'protect_self_recent', 'protect_self_max'] + protect_vars_s].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6e4c50-c5a6-42b0-b54c-ad76ac272d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#family conflict\n",
    "#Conflict Subscale from the Family Environment Scale Sum of Youth Report (RAW Score): fes_youth_q1 + fes_youth_q2 + fes_youth_q3 + fes_youth_q4 + fes_youth_q5 + fes_youth_q6 + fes_youth_q7 + fes_youth_q8 + fes_youth_q9; Validation: Minimum of five items answered\n",
    "#protect_vars_4 = ['fes_y_ss_fc_pr','fes_y_ss_fc',\n",
    "#'fes_youth_q1', 'fes_youth_q2', 'fes_youth_q3', 'fes_youth_q4', 'fes_youth_q5', 'fes_youth_q7', 'fes_youth_q8' ,'fes_youth_q9' ,'fes_youth_q10' , 'fes_youth_q11' ,'fes_youth_q12' ,'fes_youth_q13' ,'fes_youth_q14' ,'fes_youth_q15' ,'fes_youth_q16' ,'fes_youth_q17' ,'fes_youth_q18']\n",
    "\n",
    "fam_con_1= ['fes_youth_q1', 'fes_youth_q2', 'fes_youth_q3', 'fes_youth_q4', 'fes_youth_q5', 'fes_youth_q7', 'fes_youth_q8' ,'fes_youth_q9' ]\n",
    "#fam_con_2 =['fes_youth_q1', 'fes_youth_q2', 'fes_youth_q3', 'fes_youth_q4', 'fes_youth_q5', 'fes_youth_q7', 'fes_youth_q8' ,'fes_youth_q9','fes_youth_q10' , 'fes_youth_q11' ,'fes_youth_q12' ,'fes_youth_q13' ,'fes_youth_q14' ,'fes_youth_q15' ,'fes_youth_q16' ,'fes_youth_q17' ,'fes_youth_q18']\n",
    "\n",
    "ACE_df['protect_fam_1'] = ACE_df.apply(\n",
    "    lambda row: row[fam_con_1].sum() if (row['eventname'] == '2_year_follow_up_y_arm_1' and row[fam_con_1].notna().sum() >= 5) else None, \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d26f5e-b822-435a-862c-3066e4ee7f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "######PROTECTION\n",
    "#1 = Not like him/her; 2 = Somewhat like him/her; 3 = A lot like him/her\n",
    "#\t1 = True; 0 = False - sometimes switches but not in these\n",
    "###CRPBI not asked year 2....\n",
    "\n",
    "#protect checks \n",
    "#protect_vars = ['crpbi_parent1_y', 'crpbi_parent2_y' , 'crpbi_parent3_y', 'crpbi_parent4_y', 'crpbi_parent5_y', 'fes_youth_q10' , 'fes_youth_q13' ,'fes_youth_q15' ,'fes_youth_q17' ,'fes_youth_q18' , 'crpbi_caregiver12_y' ,'crpbi_caregiver13_y', 'crpbi_caregiver14_y', 'crpbi_caregiver15_y' ,'crpbi_caregiver16_y' ]\n",
    "#protect_vars2 = ['wps_ss_sum' , 'wps_ss_sum_nm' ,'pnh_ss_protective_scale' ,'pnh_encourage', 'pnh_art_involve' , 'pnh_help' , 'pnh_how_much_encourage' , 'pnh_how_much_help' , 'pnh_substance' , 'pnh_ss_protective_scale_nm' , 'crpbi_y_ss_parent_nm' ,'crpbi_y_ss_caregiver_nm', 'crpbi_y_ss_caregiver' ,'crpbi_y_ss_parent']\n",
    "\n",
    "###parent score \n",
    "# 'crpbi_y_ss_parent_nm' ,'crpbi_y_ss_caregiver_nm', 'crpbi_y_ss_caregiver' ,'crpbi_y_ss_parent' ##mean scores and number missing\n",
    "print(ACE_df['crpbi_y_ss_caregiver'].value_counts())\n",
    "print (ACE_df ['crpbi_y_ss_caregiver_nm'].value_counts()) #missing numbers\n",
    "#requires minimum of 4 answers\n",
    "\n",
    "print(ACE_df['crpbi_y_ss_parent'].value_counts())\n",
    "print (ACE_df ['crpbi_y_ss_parent_nm'].value_counts()) #missing numbers\n",
    "\n",
    "###self score\n",
    "print(ACE_df['wps_ss_sum'].value_counts())\n",
    "print (ACE_df ['wps_ss_sum_nm'].value_counts()) #missing numbers\n",
    "#also mimimmum of 4\n",
    "\n",
    "###friend score\n",
    "print(ACE_df['pnh_ss_protective_scale'].value_counts())\n",
    "print (ACE_df ['pnh_ss_protective_scale_nm'].value_counts()) #missing numbers\n",
    "#at least 3 specific questions answered\n",
    "\n",
    "#parent - baseline, year 1, year 3\n",
    "#wps_ss_sum (self) 1 year \n",
    "#friend yeare 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea5774f-f1f0-4a03-abbd-1d36a7900c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "year2_data = ACE_df[ACE_df['eventname'] == '2_year_follow_up_y_arm_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0836e404-6e4d-4678-ac6b-9cb7c1e2b9bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ace_columns = ['ACE_PA_count', 'ACE_SA_count', 'ACE_N_count', 'ACE_EA_count', 'ACE_D_count', 'ACE_MH_count', \n",
    "             'ACE_DV_count',   'ACE_F_count' , 'ACE_NV_count', 'ACE_J2_count', 'ACE_B_count', 'ACE_SS_count']\n",
    "# Calculate the total \n",
    "ACE_df[ace_columns].sum(axis=1)\n",
    "#ace_total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
